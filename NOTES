# -*- mode: org; -*-

#+OPTIONS: toc:nil f:nil
#+OPTIONS: tex:dvipng
#+LATEX_HEADER: \usepackage{mathrsfs}
# #+LATEX_HEADER: \usepackage{amsmath}
# #+LATEX_HEADER: \usepackage{amssymb}
# #+LATEX_HEADER: \usepackage{cancel}

#+TITLE:  Nanodegree Program:  Self-Driving Car
#+AUTHOR: David A. Ventimiglia
#+EMAIL: dventimi@gmail.com

* Introduction to Deep Learning Module
* Regression and Classification
* Neural Networks

  - What is the Activation?  ::

       \begin{equation}
       a = \sum_{i=1}^k X_i w_i
       \end{equation}

  - What is the Firing Threshold?  ::

       \[\theta\]

  - What are the Weights?  ::

       \[w_i\]

  - What is a Perceptron?  ::

       \begin{equation*}
       y =
       \begin{cases}
       1 & \text{if} a \ge \theta, \\
       0 & \text{if} a \lt \theta
       \end{cases}
       \end{equation*}

  - What does a Perceptron represent geometrically?  ::

       Hyper-planes

  - Why can't XOR be computed with a single Perceptron?  ::

       Because the output of XOR is not linearly-separable.

  - What are two rules for training the weights?  ::

    1. Perceptron Rule

    2. Gradient Descent Rule

  - What is the Perceptron Rule?  ::

       \[ w_i = w_i + \Delta w_i \]

       \[ \hat{y} = \left( \sum_i w_i X_i \ge 0 \right) \]

       \[ \Delta w_i = \eta \left( y-\hat{y} \right) X_i \]

       Where $y$ is the /target/, $\hat{y}$ is the /output/, and
       $\eta$ is the /learning rate/.

  - What is the Gradient Descent Rule?  ::

       \[ w_i = w_i + \Delta w_i \]

       \[ \Delta w_i = \eta \left( y-a \right) X_i \]

  - What is a nice property of the Perceptron Rule?  ::

       Finite convergence

  - What is a nice property of the Gradient Descent Rule?  ::

       Robustness

  - What is the /Sigmoid Function/?  ::

       \[ \sigma(a) = \frac{1}{1 + e^{-a}} \]

       \[ a \to -\infty, \sigma(a) \to 0 \]

       \[ a \to \infty, \sigma(a) \to 1 \]

  - What is the derivative of the /Sigmoid Function/?  ::

       \[ D \sigma(a) = \sigma(a) \left( 1-\sigma(a) \right) \]

  - What are some advanced methods for optimizing a neural network?  ::

    1. momentum

    2. higher-order derivatives

    3. randomized optimization

    4. complexity penalty

  - What are some things that contribute to neural network complexity?  ::

    1. many nodes

    2. many layers

    3. large-valued weights

  - What is /Restriction Bias/?  ::

       /Restriction Bias/ is a model's representational power and
       comprises the set of all hypotheses that we'll consider.  It
       tells you something about what the model is /able/ to
       represent.

  - How can a Boolean function be represented by a neural net?  ::

       with a network of threshold-like units

  - How can a continuous function be represented by a neural net?  ::

       with a single hidden layer

  - How can an arbitrary function be represented by a neural net?  ::

       with just two hidden layers

  - How can we counter overfitting in a neural net?  ::

       with cross-validation

  - What is /Preference Bias/?  ::

       /Preference Bias/ is an algorithm that tells you which among
       several representations to prefer.

  - How do we set the initial weights?  ::

       Typically, with small, random values.

* MiniFlow
* Introduction to TensorFlow

  - What was an important neural network of the 1980s?  ::

       Fukushima's "Neocognition"

  - What was an important neural network of the 1990s?  ::

       Le Cun's "LENET-5"

  - What was an important neural network of the 21st century?  ::

       Krizhevsky's "ALEXNET"

  - What are some important landmarks for neural networks in the 21st century?  ::

    1. 2009:  speech recognition

    2. 2012:  computer vision

    3. 2014:  machine translation

  - What are some reasons for the resurgence of popularity in neural networks?  ::

    1. availability of large data sets

    2. availability of computational resources

  - How do you represent a constant in TensorFlow?  ::

       #+BEGIN_SRC python
       tensorflow.constant()
       #+END_SRC

  - How do you represent an environment for executing a computational graph in TensorFlow?  ::

       #+BEGIN_SRC python
       tensorflow.Session().run()
       #+END_SRC

  - How do you represent input data in TensorFlow?  ::

       #+BEGIN_SRC python
       x = tensorflow.placeholder(tf.string)
       y = tensorflow.placeholder(tf.int32)
       z = tensorflow.placeholder(tf.float32)
       #+END_SRC

  - How do you feed input to set the placeholder tensor in TensorFlow?  ::

       #+BEGIN_SRC python
       tensorflow.Session().run(x, feed_dict={x: 'Hello, World!'})
       #+END_SRC

  - How do you perform arithmetic in TensorFlow?   ::

       #+BEGIN_SRC python
       x = tf.add(5, 2)
       y = tf.sub(10, 4)
       z = tf.mul(2, 5)
       t = tf.div(10, 2)
       #+END_SRC

  - What is the fundamental building block of Machine Learning?  ::

       classification

  - What is another name for a /Logistic Classifier/?  ::

       linear classifier

  - What is the mathematical form of a /Logistic Classifier/?  ::

       \[ W X + b = y \]

       Where $W$ are the /weights/, $X$ are the /input data/, $b$ are
       the /bias/, and $y$ are the /output scores/.

  - What is the /SoftMax Function/  ::

       The /SoftMax Function/ is a convenient way to turn a set of
       real numbers scores into a set of probabilities.

  - What is the mathematical form of the /SoftMax Function/?  ::

       \[ S(y_i) = \frac{e^{y_i}}{\sum_j e^{y_j}} \]

  - What is another name for the /output scores/ in /Logistic Regression/?  ::

       logits

  - How do you represent the weights and biases of a neural network in TensorFlow?  ::

       #+BEGIN_SRC python
       x = tensorflow.Variable(5)
       #+END_SRC

  - How do you initialize the state of a ~tensorflow.Variable()~ in TensorFlow?  ::

       #+BEGIN_SRC python
       tensorflow.Session().run(tensorflow.initialize_all_variables())
       #+END_SRC

  - Why do you initialize the state of a ~tensorflow.Variable()~ in TensorFlow?  ::

       because this tensor stores its state in the session

  - How do you generate random numbers over a normal distribution in TensorFlow?  ::

       #+BEGIN_SRC python
       n_features = 120
       n_labels = 5
       weights = tensorflow.Variable(tensorflow.truncated_normal((n_features, n_labels)))
       #+END_SRC

  - How do you create a tensor of zeros in TensorFlow?  ::

       #+BEGIN_SRC python
       tensorflow.zeros(5)
       #+END_SRC

  - How do you create an operation for the /SoftMax Function/ in TensorFlow?  ::

       #+BEGIN_SRC python
       x = tensorflow.nn.softmax([2.0, 1.0, 0.2])
       #+END_SRC

  - How does scaling up the output scores (/logits/) of your classifier affect the output probabilities?  ::

       It makes the largest approach 1 and all the others approach 0,
       which means that your classifier is more confident about its
       prediction.

  - How does scaling down the output scores (/logits/) of your classifier affect the output probabilities?  ::

       It makes all of them approach 0, which means that your
       classifier is less confident about its prediction.

  - What is /One-Hot Encoding/? ::

       /One-Hot Encoding/ represents your labels as a vector, with as
       many elements as there are output classes, with a 1 for the
       correct label and a 0 for all the others.

  - When does /One-Hot Encoding/ break down?  ::

       when the number of classes becomes very large

  - How do we measure the distance between the label probability vector and the classifier's output probability vector?  ::

       with /cross-entropy/?

  - What is /Cross-Entropy/?  ::

       \[ D(S, L) = - \sum_i L_i \log(S_i) \]

       where $S = S(y)$ is the /SoftMax Function/ vector and $L$ is
       the label probability vector.

  - What are the components of a linear classifier in schematic form?  ::

       #+BEGIN_SRC ditaa :file linear-model.png
       +--------------------------------------+
       |  x                                   |
       |  Input                               |
       +--------------------------------------+
                         | Wx + b 
                         V
       +--------------------------------------+
       |  y                                   |
       |  Logit                               |
       +--------------------------------------+
                         | S(y)
                         V
       +--------------------------------------+
       |  x                                   |
       |  SoftMax                             |
       +--------------------------------------+
                         | D(S,L)
                         V
       +--------------------------------------+
       |  x                                   |
       |  One-Hot Labels                      |
       +--------------------------------------+
       #+END_SRC

  - What is the mathematical form of /Multinomial Logistic Regression/?  ::

       \[ D(S(W X + b), L) \]

  - How do you represent the /Cross-Entropy Function/ in TensorFlow?  :: 

       #+BEGIN_SRC python
       import tensorflow as tf
       softmax_data = [0.7, 0.2, 0.1]
       one_hot_data = [1.0, 0.0, 0.0]
       softmax = tf.placeholder(tf.float32)
       one_hot = tf.placeholder(tf.float32)
       cross_entropy = -tf.reduce_sum(tf.mul(one_hot, tf.log(softmax)))
       output = tf.Session().run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data})
       #+END_SRC

  - What is an example of a /Loss Function/ in mathematical form?   :: 

       \[ \mathscr{L}(W, b) = \frac{1}{N} \sum_i D(S(W X_i + b), L_i) \]

  - What are two guiding principles for achieving numerical stability in gradient descent?  :: 

    1. variables have zero mean:  $\langle X_i \rangle = 0$

    2. variables have equal variance:  $\sigma(X_i) = \sigma(X_j)$

  - What does /badly-conditioned/ mean for optimization problems?  ::

       /Badly-conditioned/ means that the optimizer has to search a
       lot for the optimum.

  - How do you condition your variables when dealing with RGB images?  :: 

       Compute the scaled quantities for each pixel:

       \[ \frac{R-128}{128}, \frac{G-128}{128}, \frac{B-128}{128} \]

  - How do you condition your weights and biases?  :: 

       Draw them randomly from a Gaussian distribution with a mean of
       0 and a standard deviation of $\sigma$.

  - What are the steps of the optimization loop?  :: 

    1. \[ w \gets w - \alpha \Delta_w \mathscr{L} \]

    2. \[ b \gets b - \alpha \Delta_b \mathscr{L} \]

  - How do you guard against your classifier memorizing your training data?  ::

       Select a portion of your training data and set it aside as test
       data.

  - How do you guard against your classifier learning about your test data?  :: 

       Select another portion of your training data and set it aside
       as validation data.

  - What is the "Rule of 30"?  :: 

       A change that affects 30 examples in your validation set is
       usually statistically significant, and typically can be
       trusted.

  - What is a common minimum size validation set?  :: 

       30,000 examples which yields better than 0.1% accuracy.

  - When is this heuristic typically invalid?  :: 

       When your training sample is not /well-balanced/?

  - What does /well-balanced/ mean in the context of training set sizes?  :: 

       /Well-balanced/ means that all of the classes occur with
       approximately equal frequency within the training set.

  - What is the best way to cope with training data that are not /well-balanced/?  :: 

       Get more data.

  - What is another way to cope with training data that are not /well-balanced/, when getting more data is not an option?  :: 

       /Cross-validation/

  - What is one problem with /Cross-validation/?  :: 

       /Cross-validation/ is often a slow process.

  - What is a fundamental problem with Gradient Descent?  :: 

       Gradient descent is difficult to scale.

  - What is the typical ratio of computational operations for the gradient of a loss function relative to the loss function itself?  :: 

       3:1

  - What is an alternative to Gradient Descent that has better scaling properties?  :: 

       Stochastic Gradient Descent

  - What is /Stochastic Gradient Descent/?  :: 

       /Stochastic Gradient Descent/ (SGD) computes the average loss
       function over a random sample of the training data at each
       step, rather than over all of the data at each step, when
       computing the derivative.

  - How many training samples are typically in a training set in Stochastic Gradient Descent?  :: 

       Between 1 and 1000

  - How do we compensate for the bad derivative estimates of SGD?  :: 

       by taking very small steps

  - How do we incorporate /momentum/ in SGD?  :: 

       Keep a running average of the gradients, rather than the most
       current gradient estimate.

  - What is /learning rate decay/ in SGD:  :: 

       Make the learning rate smaller as you train.

  - Which is better, a larger learning rate or a smaller learning rate?  :: 

       While there's no one size that fits all, often a smaller
       learning rate actually will produce a better, more accurate
       classifier than a larger learning rate will.

  - What are some common hyper-parameters in optimizing a linear classifier?  :: 

    1. initial learning rate

    2. learning rate decay

    3. momentum

    4. batch size

    5. weight initialization

  - What is ADAGRAD?  ::

       ADAGRAD is a modification of SGD that implicitly does momentum
       and learning rate decay.  It's often easier and more robust
       than plain SGD, but it's also often a little worse than
       precisely-tuned SGD.

  - What is /mini-batching/?  :: 

       /Mini-batching/ is a technique for training on subsets of the
       data instead of on all of the data.

  - What is an advantage of mini-batching?  :: 

       Mini-batching provides the ability to train a model even if a
       computer lacks the memory to store the complete data set.

  - What is a disadvantage of mini-batching?   :: 

       Mini-batching is inefficient, since you cannot calculate the
       loss simultaneously across all of the samples.

  - How do you implement mini-batching in TensorFlow?  :: 

       #+BEGIN_SRC python
       # Features and Labels
       features = tensorflow.placeholder(tensorflow.float32, [None, n_input])
       labels = tensorflow.placeholder(tensorflow.float32, [None, n_classes])       
       #+END_SRC

  - What is an /epoch/?  :: 

       An /epoch/ is a single forward and backward pass of the whole
       data-set, that is used to increase the accuracy of the model
       without requiring more data.

* Deep Neural Networks

  - What is a /RELU/?  ::

       A /RELU/ is a /Rectified Linear Unit/, and is a very simple
       non-linear function.

       \begin{equation*}
       y(x) = 
       \begin{cases}
       0, & x \le 0 \\
       x, & x \gt 0
       \end{cases}
       \end{equation*}

  - What is the derivative of a RELU?  :: 

       \begin{equation*}
       y(x) = 
       \begin{cases}
       0, & x \le 0 \\
       1, & x \gt 0
       \end{cases}
       \end{equation*}

  - What is the typical ratio of resources used (compute + memory) for the back-propagation step relative to the forward-propagation step in a modern neural network framework?  :: 

       2:1

  - What is the central idea of /Deep Neural Networks/?  :: 

       Add more hidden layers rather than adding more neurons within
       the hidden layers.

  - What are some reasons to prefer deeper networks over wider networks?  :: 

    1. Deep models tend to have greater computational efficiency.

    2. Deep models naturally capture hierarchical structure in the
       phenomena they model.

  - How do you save the weights and biases of a model in TensorFlow?  ::

       #+BEGIN_SRC python
       saver = tensorflow.train.Saver()
       sess = tensorflow.Session()
       # Steps to train build and train the model
       saver.save(sess, 'model.ckpt')
       #+END_SRC

  - How do you restore the weights and biases of a saved model in TensorFlow?  :: 

       #+BEGIN_SRC python
       # Remove the previous weights and bias
       tensorflow.reset_default_graph()
       # Two Variables: weights and bias (example)
       weights = tensorflow.Variable(tensorflow.truncated_normal([2, 3]))
       bias = tensorflow.Variable(tensorflow.truncated_normal([3]))
       saver = tensorflow.train.Saver()
       sess = tensorflow.Session()
       saver.restore(sess, save_file)
       #+END_SRC

  - How do you cope with naming errors when restoring weights and biases into a new model?  :: 

       Use the TensorFlow ~name~ named parameter when invoking
       ~tensorflow.Variable~ to create the weights and biases, and use
       the same names in the old model and in the new model.

  - What are two reasons that deep models became effective only relatively recently?  :: 

    1. Deep models are effective only with relatively large training
       sets.  Large training sets only became available to the
       academic community relatively recently.

    2. Regularization

  - What are two ways to prevent overfitting of a deep model?  ::

    1. /Early Termination/

    2. /Regularization/

  - What is /Early Termination/?  ::

       /Early Termination/ is when you stop training as soon as we
       stop improving performance on the validation set.

  - What is /Regularization/? :: 

       /Regularization/ is applying artificial constraints on your
       network that implicitly reduce the number of free parameters
       while not making it more difficult to optimize.

  - What are three types of regularization used in deep models?  ::

    1. /L_2 Regularization/

    2. /Dropout/

  - What is /L_2 Regularization/?  :: 

       /L_2 Regularization/ adds another term to the /loss/, which
       penalizes large weights, typically by adding a termo containing
       the /L_2 norm/ of the weights.

       \[ \mathscr{L}' = \mathscr{L} + \beta \frac{1}{2} \|W\|_2^2 \]

  - What is the /L_2 norm/?  :: 

       The /L_2 norm/ is the sum of the squares of the individual
       elements in a vector, multipled by 1/2.

  - What is the derivative of the /L_2 norm/ of a vector W?  :: 

       W

  - What is /Dropout/?  :: 

       /Dropout/ is the practice of randomly setting some of the
       activations (e.g., half of them) to 0, for each example that
       trains the network, and scale the remaining activations by the
       inverse factor (e.g., twice).  It promotes robustness by
       forcing the network to learn redundant representations.

  - What must you typically resort to doing if dropout fails to improve your deep network's performance?  :: 

       Make a bigger model.

  - How do you properly average the activations of a network trained with dropout, when performing evaluation?  :: 

       Simply remove the dropouts and the corresponding scaling
       factor.

  - How do you perform dropout in TensorFlow?  :: 

       #+BEGIN_SRC python
       import tensorflow as tf
       keep_prob = tf.placeholder(tf.float32) # probability to keep units
       hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])
       hidden_layer = tf.nn.relu(hidden_layer)
       hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)
       logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])       
       #+END_SRC

* Convolutional Neural Networks

  - What is /weight sharing/?  ::

       /Weight sharing/ the sharing of inputs and the joint training
       of their weights when you know /a priori/ that they contain the
       same kind of information.  They help preserve statistical
       invariants, such as translation invariance.

  - How does weight sharing manifest in deep neural networks for images?  :: 

       /Convolutional Neural Networks/

  - How does weight sharing manifest in deep neural networks for text and sequences in general?  :: 

       /Embeddings/ and /Recurrent Neural Networks/

  - What are /Convolutional Neural Networks/ :: 

       /Convolutional Neural Networks/---or /Convnets/---perform
       weight sharing across spacial dimensions, such as for images.

  - What is a /convolution/ in a Convnet?  :: 

       A /convolution/ is a small neural network operating on a small
       patch of an image, whereby the patch "slides" over the image,
       mapping the network output to a new image.

  - In a convnet pyramid, how do the spatial extent and the depth of each layer trend, from the base (input) to the tip (output)?  :: 

       The spatial extent shrinks while the depth increases.

  - What does the depth represent in a layer of a convnet?  :: 

       semantic representation

  - What is a /kernel/ in a convnet?  :: 

       /Kernal/ is just another name for patch.

  - What is a /feature map/ in a convnet?  :: 

       A /feature map/ is just another name the individual layers
       within a convnet layer, along the depth dimension.

  - What is a /filter/ in a convnet?  :: 

       /Filter/ is just another name for patch or kernal.

  - What is /stride/ in a convnet?  :: 

       /Stride/ refers to the number of pixels that you're shifting
       each time you move your filter.

  - What is /Valid Padding/ in a convnet?  :: 

       /Valid Padding/ is when your patch does not shift off the edge
       of the input image.

  - What is /Same Padding/ in a convnet?  :: 

       /Same Padding/ is when your patch does shift off the edge of
       the input image, which case the input image is padded with
       sufficient zeros around its border.

  - Given our input layer with volume W, a filter with volume F, a stride of S, and a padding of P, what is the volume of the next layer?  :: 

       \[ \frac{W-F+2P}{S+1} \]

  - Given in input layer, what are the dimensions of the output layer?  ::

       ~new_height = (input_height - filter_height + 2 * P)/S + 1~

       ~new_width = (input_width - filter_width + 2 * P)/S + 1~

  - How do you create a convolutional layer in TensorFlow?  :: 

       #+BEGIN_SRC python
       import tensorflow as tf
       k_output = 64
       image_width = 10
       image_height = 10
       color_channels = 3
       filter_size_width = 5
       filter_size_height = 5
       # Input/Image
       input = tf.placeholder(
           tf.float32,
           shape=[None, image_width, image_height, color_channels])
       # Weight and bias
       weight = tf.Variable(tf.truncated_normal(
           [filter_size_width, filter_size_height, color_channels, k_output]))
       bias = tf.Variable(tf.zeros(k_output))
       conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')
       conv_layer = tf.nn.bias_add(conv_layer, bias)
       conv_layer = tf.nn.relu(conv_layer)
       #+END_SRC

  - What are three ways to improve upon a simple convnet?  :: 

    1. /Pooling/

    2. /1x1 Convolutions/

    3. /Inception/

  - What is /Pooling/?  ::

       Instead of reducing the spatial dimension from one layer to the
       next simply with a large stride value, /pooling/ instead uses a
       small stride value, but then combines the output of all of the
       patches within a region in order to reduce the spatial
       dimension.

  - What are two kinds of pooling?  :: 

    1. /Max Pooling/

    2. /Average Pooling/

  - What is /Max Pooling/?  ::

       At every point in a feature map, look at a small neighborhood
       around that point---on the feature map---and compute the
       maximum of all the responses around it.

  - What are the advantages of Max Pooling?  :: 

    1. does not add additional parameters

    2. more accurate and more resistant to overfitting

  - What are the disadvantages of Max Pooling?  ::

    1. does add more hyper-parameters (pooling region size, pooling
       stride)

    2. more expensive model to compute

  - What is /Average Pooling/?  ::

       Rather than take the maximum within a neighborhood of a pixel
       in a feature map, instead take the average over that
       neighborhood.

  - How do you perform Max Pooling in TensorFlow?  ::

       #+BEGIN_SRC python
       import tensorflow as tf
       conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')
       conv_layer = tf.nn.bias_add(conv_layer, bias)
       conv_layer = tf.nn.relu(conv_layer)
       conv_layer = tf.nn.max_pool(
           conv_layer,
           ksize=[1, 2, 2, 1],
           strides=[1, 2, 2, 1],
           padding='SAME')       
       #+END_SRC

  - What is a very typical architecture for a convnet?  ::

       #+BEGIN_SRC ditaa :file convnet-arch.png
       +--------------------------------------+
       +cRED         Classifier               +
       +--------------------------------------+
       +cYEL         Fully Connected          +
       +--------------------------------------+
       +cYEL         Fully Connected          +
       +--------------------------------------+
       +cRED         Max Pooling              +
       +--------------------------------------+
       +cGRE         Convolution              +
       +--------------------------------------+
       +cRED         Max Pooling              +
       +--------------------------------------+
       +cGRE         Convolution              +
       +--------------------------------------+
       +cBLU         Image                    +
       +--------------------------------------+
       #+END_SRC

  - What is a /1x1 Convolution/?  :: 

       A /1x1 Convolution/ is an inexpensive way of adding additional
       representational power to a convnet.

  - What is a powerful general strategy for combining these techniques for convnets?  :: 

       An /Inception Module/

  - What is an /Inception Module/ :: 

       Each layer concatenates the output of following sub-layers.

    1. Average Pooling followed by 1x1

    2. 1x1

    3. 1x1 followed by 3x3

    4. 1x1 followed by 5x5

#  LocalWords:  ge lt infty MiniFlow TensorFlow Fukushima's Cun's tf
#  LocalWords:  Neocognition LENET Krizhevsky's ALEXNET tensorflow Wx
#  LocalWords:  mul SoftMax ditaa png Logit softmax mathscr langle
#  LocalWords:  rangle RGB SGD ADAGRAD
