# -*- mode: org; -*-

* Introduction to Deep Learning Module
* Regression and Classification
* Neural Networks

  - What is the Activation?  ::

       \begin{equation}
       a = \sum_{i=1}^k X_i w_i
       \end{equation}

  - What is the Firing Threshold?  ::

       \[\theta\]

  - What are the Weights?  ::

       \[w_i\]

  - What is a Perceptron?  ::

       \begin{equation*}
       y =
       \begin{cases}
       1 & if a \ge \theta, \\
       0 & if a \lt \theta
       \end{cases}
       \end{equation*}

  - What does a Perceptron represent geometrically?  ::

       Hyperplanes

  - Why can't XOR be computed with a single Perceptron?  ::

       Because the output of XOR is not linearly-separable.

  - What are two rules for training the weights?  ::

    1. Perceptron Rule

    2. Gradient Descent Rule

  - What is the Perceptron Rule?  ::

       \[ w_i = w_i + \Delta w_i \]

       \[ \hat{y} = \left( \sum_i w_i X_i \ge 0 \right) \]

       \[ \Delta w_i = \eta \left( y-\hat{y} \right) X_i \]

       Where $y$ is the /target/, $\hat{y}$ is the /output/, and
       $\eta$ is the /learning rate/.

  - What is the Gradient Descent Rule?  ::

       \[ w_i = w_i + \Delta w_i \]

       \[ \Delta w_i = \eta \left( y-a \right) X_i \]

  - What is a nice property of the Perceptron Rule?  ::

       Finite convergence

  - What is a nice property of the Gradient Descent Rule?  ::

       Robustness

  - What is the /Sigmoid Function/?  ::

       \[ \sigma(a) = \frac{1}{1 + e^{-a}} \]

       \[ a \to -\infty, \sigma(a) \to 0 \]

       \[ a \to \infty, \sigma(a) \to 1 \]

  - What is the derivative of the /Sigmoid Function/?  ::

       \[ D \sigma(a) = \sigma(a) \left( 1-\sigma(a) \right) \]

  - What are some advanced methods for optimizing a neural network?  ::

    1. momentum

    2. higher-order derivatives

    3. randomized optimization

    4. complexity penalty

  - What are some things that contribute to neural network complexity?  ::

    1. many nodes

    2. many layers

    3. large-valued weights

  - What is /Restriction Bias/?  ::

       /Restriction Bias/ is a model's representational power and
       comprises the set of all hypotheses that we'll consider.  It
       tells you something about what the model is /able/ to
       represent.

  - How can a Boolean function be represented by a neural net?  ::

       with a network of threshold-like units

  - How can a continuous function be represented by a neural net?  ::

       with a single hidden layer

  - How can an arbitrary function be represented by a neural net?  ::

       with just two hidden layers

  - How can we counter overfitting in a neural net?  ::

       with cross-validation

  - What is /Preference Bias/?  ::

       /Preference Bias/ is an algorithm that tells you which among
       several representations to prefer.

  - How do we set the initial weights?  ::

       Typically, with small, random values.

* MiniFlow
* Introduction to TensorFlow

  - What was an important neural network of the 1980s?  ::

       Fukushima's "Neocognition"

  - What was an important neural network of the 1990s?  ::

       Le Cun's "LENET-5"

  - What was an important neural network of the 21st century?  ::

       Krizhevsky's "ALEXNET"

  - What are some important landmarks for neural networks in the 21st century?  ::

    1. 2009:  speech recognition

    2. 2012:  computer vision

    3. 2014:  machine translation

  - What are some reasons for the resurgence of popularity in neural networks?  ::

    1. availability of large data sets

    2. availability of computational resources

  - How do you represent a constant in TensorFlow?  ::

       #+BEGIN_SRC python
       tensorflow.constant()
       #+END_SRC

  - How do you represent an environment for executing a computational graph in TensorFlow?  ::

       #+BEGIN_SRC python
       tensorflow.Session().run()
       #+END_SRC

  - How do you represent input data in TensorFlow?  ::

       #+BEGIN_SRC python
       x = tensorflow.placeholder(tf.string)
       y = tensorflow.placeholder(tf.int32)
       z = tensorflow.placeholder(tf.float32)
       #+END_SRC

  - How do you feed input to set the placeholder tensor in TensorFlow?  ::

       #+BEGIN_SRC python
       tensorflow.Session().run(x, feed_dict={x: 'Hello, World!'})
       #+END_SRC

  - How do you perform arithmetic in TensorFlow?   ::

       #+BEGIN_SRC python
       x = tf.add(5, 2)
       y = tf.sub(10, 4)
       z = tf.mul(2, 5)
       t = tf.div(10, 2)
       #+END_SRC

  - What is the fundamental building block of Machine Learning?  ::

       classification

  - What is another name for a /Logistic Classifier/?  ::

       linear classifer

  - What is the mathematical form of a /Logistic Classifier/?  ::

       \[ W X + b = y \]

       Where $W$ are the /weights/, $X$ are the /input data/, $b$ are
       the /bias/, and $y$ are the /output scores/.

  - What is the /SoftMax Function/  ::

       The /SoftMax Function/ is a convenient way to turn a set of
       real numbers scores into a set of probabilities.

  - What is the mathematical form of the /SoftMax Function/?  ::

       \[ S(y_i) = \frac{e^{y_i}}{\sum_j e^{y_j}} \]

  - What is another name for the /output scores/ in /Logistic Regression/?  ::

       logits

  - How do you represent the weights and biases of a neural network in TensorFlow?  ::

       #+BEGIN_SRC python
       x = tensorflow.Variable(5)
       #+END_SRC

  - How do you initialize the state of a ~tensorflow.Variable()~ in TensorFlow?  ::

       #+BEGIN_SRC python
       tensorflow.Session().run(tensorflow.initialize_all_variables())
       #+END_SRC

  - Why do you initialize the state of a ~tensorflow.Variable()~ in TensorFlow?  ::

       because this tensor stores its state in the session

  - How do you generate random numbers over a normal distribution in TensorFlow?  ::

       #+BEGIN_SRC python
       n_features = 120
       n_labels = 5
       weights = tensorflow.Variable(tensorflow.truncated_normal((n_features, n_labels)))
       #+END_SRC

  - How do you create a tensor of zeros in TensorFlow?  ::

       #+BEGIN_SRC python
       tensorflow.zeros(5)
       #+END_SRC

  - How do you create an operation for the /SoftMax Function/ in TensorFlow?  ::

       #+BEGIN_SRC python
       x = tensorflow.nn.softmax([2.0, 1.0, 0.2])
       #+END_SRC

  - How does scaling up the output scores (/logits/) of your classifier affect the output probabilities?  ::

       It makes the largest approach 1 and all the others approach 0,
       which means that your classifier is more confident about its
       prediction.

  - How does scaling down the output scores (/logits/) of your classifier affect the output probabilities?  ::

       It makes all of them approach 0, which means that your
       classifier is less confident about its prediction.

  - What is /One-Hot Encoding/? ::

       /One-Hot Encoding/ represents your labels as a vector, with as
       many elements as there are output classes, with a 1 for the
       correct label and a 0 for all the others.

  - When does /One-Hot Encoding/ break down?  ::

       when the number of classes becomes very large

  - How do we measure the distance between the label probability vector and the classifier's output probability vector?  ::

       with /cross-entropy/?

  - What is /Cross-Entropy/?  ::

       \[ D(S, L) = - \sum_i L_i \log(S_i) \]

       where $S = S(y)$ is the /SoftMax Function/ vector and $L$ is
       the label probability vector.

  - What are the components of a linear classifier in schematic form?  ::

       #+BEGIN_SRC ditaa :file linear-model.png
       +--------------------------------------+
       |  x                                   |
       |  Input                               |
       +--------------------------------------+
                         | Wx + b 
                         V
       +--------------------------------------+
       |  y                                   |
       |  Logit                               |
       +--------------------------------------+
                         | S(y)
                         V
       +--------------------------------------+
       |  x                                   |
       |  SoftMax                             |
       +--------------------------------------+
                         | D(S,L)
                         V
       +--------------------------------------+
       |  x                                   |
       |  One-Hot Labels                      |
       +--------------------------------------+
       #+END_SRC

  - What is the mathematical form of /Multinomial Logistic Regression/?  ::

       \[ D(S(W X + b), L) \]

  - How do you represent the /Cross-Entropy Function/ in TensorFlow?  :: 

       #+BEGIN_SRC python
       import tensorflow as tf
       softmax_data = [0.7, 0.2, 0.1]
       one_hot_data = [1.0, 0.0, 0.0]
       softmax = tf.placeholder(tf.float32)
       one_hot = tf.placeholder(tf.float32)
       cross_entropy = -tf.reduce_sum(tf.mul(one_hot, tf.log(softmax)))
       output = tf.Session().run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data})
       #+END_SRC

  - What is an example of a /Loss Function/ in mathematical form?   :: 

       \[ \mathscr{L}(W, b) = \frac{1}{N} \sum_i D(S(W X_i + b), L_i) \]

  - What are two guiding principles for achieving numerical stability in gradient descent?  :: 

    1. variables have zero mean:  $\langle X_i \rangle = 0$

    2. variables have equal variance:  $\sigma(X_i) = \sigma(X_j)$

  - What does /badly-conditioned/ mean for optimization problems?  ::

       /Badly-conditioned/ means that the optimizer has to search a
       lot for the optimum.

  - How do you condition your variables when dealing with RGB images?  :: 

       Compute the scaled quantities for each pixel:

       \[ \frac{R-128}{128}, \frac{G-128}{128}, \frac{B-128}{128} \]

  - How do you condition your weights and biases?  :: 

       Draw them randomly from a Guassian distribution with a mean of
       0 and a standard deviation of $\sigma$.

  - What are the steps of the optimization loop?  :: 

    1. \[ w \gets w - \alpha \Delta_w \mathscr{L} \]

    2. \[ b \gets b - \alpha \Delta_b \mathscr{L} \]

  - How do you guard against your classifier memorizing your training data?  ::

       Select a portion of your training data and set it aside as test
       data.

  - How do you guard against your classifier learning about your test data?  :: 

       Select another portion of your training data and set it aside
       as validation data.

  - What is the "Rule of 30"?  :: 

       A change that affects 30 examples in your validation set is
       usually statistically significant, and typically can be
       trusted.

  - What is a common minimium size validation set?  :: 

       30,000 examples which yields better than 0.1% accuracy.

  - When is this heuristic typically invalid?  :: 

       When your training sample is not /well-balanced/?

  - What does /well-balanced/ mean in the context of training set sizes?  :: 

       /Well-balanced/ means that all of the classes occur with
       approximately equal frequency within the training set.

  - What is the best way to cope with training data that are not /well-balanced/?  :: 

       Get more data.

  - What is another way to cope with training data that are not /well-balanced/, when getting more data is not an option?  :: 

       /Cross-validation/

  - What is one problem with /Cross-validation/?  :: 

       /Cross-validation/ is often a slow process.

  - What is a fundamental problem with Gradient Descent?  :: 

       Gradient descent is difficult to scale.

  - What is the typical ratio of computational operations for the gradient of a loss function relative to the loss function itself?  :: 

       3:1

  - What is an alternative to Gradient Descent that has better scaling properties?  :: 

       Stochastic Gradient Descent

  - What is /Stochastic Gradient Descent/?  :: 

       /Stochastic Gradient Descent/ (SGD) computes the average loss
       function over a random sample of the training data at each
       step, rather than over all of the data at each step, when
       computing the derivative.

  - How many training samples are typically in a training set in Stochastic Gradient Descent?  :: 

       Between 1 and 1000

  - How do we compensate for the bad derivative estimates of SGD?  :: 

       by taking very small steps

  - How do we incorporate /momentum/ in SGD?  :: 

       Keep a running average of the gradients, rather than the most
       current gradient estimate.

  - What is /learning rate decay/ in SGD:  :: 

       Make the learning rate smaller as you train.

  - Which is better, a larger learning rate or a smaller learning rate?  :: 

       While there's no one size that fits all, often a smaller
       learning rate actually will produce a better, more accurate
       classifier than a larger learning rate will.

  - What are some common hyper-parameters in optimizing a linear classifier?  :: 

    1. initial learning rate

    2. learning rate decay

    3. momentum

    4. batch size

    5. weight initialization

  - What is ADAGRAD?  ::

       ADAGRAD is a modification of SGD that implicitly does momentum
       and learning rate decay.  It's often easier and more robust
       than plain SGD, but it's also often a little worse than
       precisely-tuned SGD.

  - What is /mini-batching/?  :: 

       /Mini-batching/ is a technique for training on subsets of the
       data instead of on all of the data.

  - What is an advantage of mini-batching?  :: 

       Mini-batching provides the ability to train a model even if a
       computer lacks the memory to store the complete data set.

  - What is a disadvantage of mini-batching?   :: 

       Mini-batching is ineficient, since you cannot calculate the
       loss simultaneously across all of the samples.

  - How do you implement mini-batching in TensorFlow?  :: 

       #+BEGIN_SRC python
       # Features and Labels
       features = tensorflow.placeholder(tensorflow.float32, [None, n_input])
       labels = tensorflow.placeholder(tensorflow.float32, [None, n_classes])       
       #+END_SRC

  - What is an /epoch/?  :: 

       An /epoch/ is a single forward and backward pass of the whole
       dataset, that is used to increase the accuracy of the model
       without requiring more data.

* Deep Neural Networks

* Convolutional Neural Networks
